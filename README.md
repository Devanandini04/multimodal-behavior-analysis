# ğŸ“Œ Extraction of Gesture Features

This repository contains the complete implementation of the project **â€œExtraction of Gesture Featuresâ€**.

The project focuses on building an **end-to-end deep learning pipeline** to automatically extract, classify, and analyze human gestures from videos and correlate them with speech-related information at a frame level.

---

## ğŸ¯ Project Objectives

The primary goals of this project are:

- Develop a robust **gesture extraction pipeline** using deep learning techniques.
- Detect and classify **human gestures** from video frames.
- Identify and associate:
  - the **type of gesture**
  - the **spoken words**
  - the **speaker**
  - the **speech type**
  within the same temporal frame.
- Perform **cross-parameter analysis** to uncover meaningful patterns between gestures and speech.
- Generate structured outputs for further research and visualization.

---

## ğŸ§  Key Features

- Frame-level gesture detection and classification  
- Multimodal alignment between **gesture and speech**
- Speaker and speech-type identification
- Automated analysis pipeline for behavioral insights
- Supports both **visual** and **tabular** outputs

---

## ğŸ“¤ Output

The system produces one of the following outputs:

- ğŸ¥ **Annotated video output** with overlaid gesture, speaker, and speech information  
- ğŸ“Š **Comprehensive dataframe** containing frame-wise details such as:
  - gesture type  
  - spoken word  
  - speaker identity  
  - speech category  

---

## ğŸ—‚ï¸ Project Structure

```text
src/
â”‚
â”œâ”€â”€ model/        # Deep learning models for gesture and speech processing
â”œâ”€â”€ workflow/     # Main pipeline for integrating models and overlaying outputs
â”œâ”€â”€ Models/       # Pre-trained and fine-tuned models
